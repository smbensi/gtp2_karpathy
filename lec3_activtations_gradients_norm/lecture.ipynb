{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch normalization\n",
    "\n",
    "make possible to train DNNs quite reliably \n",
    "You want that the hidden state pre-activation  to be roughly gaussian -> the tanh won't be saturated\n",
    "So take the hidden state and normalize it to be Gaussian\n",
    "WE ARE USING IT TO CONTROL THE ACTIVATION IN THE NN \n",
    "\n",
    "We want this to be Gaussian only at initialization and not always . We want the NN to move this around to pentially make it  more diffuse, more sharp . So they added the scale and shift\n",
    "\n",
    "There is coupling between the examples in the batch. But it plays a role of a regularizer\n",
    "\n",
    "the momentum parameter is for the running mean and the running std \n",
    "The affine parameter is form the shift and scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n"
     ]
    }
   ],
   "source": [
    "# download the names.txt file from github\n",
    "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "\n",
    "mean = hpreact.mean(0, keepdim=True) # for example [32, 200] -> [1,200]  we get the mean over all the batches example \n",
    "std = hpreact.std(0, keepdim=True)\n",
    "hpreact = bngain * (hpreact - mean) / std  +  bnbias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate the batch norm layer at the end of the training\n",
    "\n",
    "with torch.no_grad():\n",
    "    # pass the training set through\n",
    "    emb = C[Xtr]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    # measure the mean and std over the entire training set\n",
    "    bnmean = hpreact.mean(0, keepdim=True)\n",
    "    bnstd = hpreact.std(0, keepdim=True)\n",
    "    \n",
    "    # So at test time we are fixing the mean and std of the batch norm layerÂ§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train a deep neural network\n",
    "\n",
    "class Linear:\n",
    "    \n",
    "    def __init__(self, fan_in, fan_out, bias=True) -> None:\n",
    "        self.weight = torch.randn((fan_out, fan_in), generator=g) / fan_in**0.5\n",
    "        self.bias = torch.zeros(fan_out) if bias else None\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weight] +([] if self.bias is None else [self.bias])\n",
    "    \n",
    "class BatchNorm1d:\n",
    "    \n",
    "    def __init__(self, dim, momentum=0.1, eps=1e-5) -> None:\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "        self.training = True\n",
    "        # parameters (trained with backprop)\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        # buffers (trained with a 'momentum update')\n",
    "        # self.register_buffer('running_mean', torch.zeros(dim))\n",
    "        # self.register_buffer('running_var', torch.ones(dim))\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        # calculate the forward pass\n",
    "        if self.training:\n",
    "            xmean = x.mean(0, keepdim=True)\n",
    "            xvar = x.var(0, keepdim=True)\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "        x_hat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "        self.out = self.gamma * x_hat + self.beta\n",
    "        # update the buffers\n",
    "        if self.training:\n",
    "            with torch.no_grad(): # exponential moving average\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "    \n",
    "\n",
    "class Tanh:\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "\n",
    "n_embd = 10 # the dimensionality of the character vectors\n",
    "n_hidden = 100 # the number of neurons in the hidden layer of the MLP\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "layers = [\n",
    "    Linear(n_embd * block_size, n_hidden),  Tanh(),\n",
    "    Linear(n_hidden, n_hidden),             Tanh(),\n",
    "    Linear(n_hidden, n_hidden),             Tanh(),\n",
    "    Linear(n_hidden, n_hidden),             Tanh(),\n",
    "    Linear(n_hidden, n_hidden),             Tanh(),\n",
    "    Linear(n_hidden, vocab_size)\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    # last layer : make less confident\n",
    "    layers[-1].weight *= 0.1\n",
    "    # all other layers : apply gain\n",
    "    for layer in layers[:-1]:\n",
    "        if isinstance(layer, Linear):\n",
    "            layer.weight *= 5/3\n",
    "            \n",
    "parameters = [C] + [param for layer in layers for param in layer.parameters()]\n",
    "print(sum(param.numel() for param in parameters)) # number of parameters in  total\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
